<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yangyi Huang</title>

    <meta name="author" content="Yangyi Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/svg" href="images/icon.svg">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-4ML2Y9ZT9M"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-4ML2Y9ZT9M');
    </script>

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yangyi Huang (黄洋逸)
                </p>
                <p>I am currently a research assistant at Westlake University. In 2024, I received my Master's degree at <a href="http://www.cad.zju.edu.cn/english.html">State Key Lab of CAD&CG</a>, <a href="http://www.en.cs.zju.edu.cn/">College of Computer Science and Technology</a>  at <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>, advised by <a href="http://www.cad.zju.edu.cn/home/dengcai/">Prof. Deng Cai</a>. 
                </p>
                <p>Previously, I have had the opportunity to work closely with <a href="https://xyyhw.top/">Hongwei Yi</a>, <a href="https://xiuyuliang.cn/">Yuliang Xiu</a> and <a href="https://justusthies.github.io/">Justus Thies</a> on exciting research projects involving the generation and reconstruction of digital humans.
                </p>
                <p>
                  Before that, I received my B.Eng(2017.09 - 2021.06) in Computer Science from the same department with an honor degree at Mixed Class, Chu Kochen Honor College. During my undergraduate studies, I enjoyed the challenges of competitive programming and participated in events such as ACM-ICPC and CCPC.
                </p>
                <p style="text-align:center">
                  <a href="mailto:huangyangyi@zju.edu.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/huangyangyi-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MzGqlqIAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/yangyi_huang_cn">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/huangyangyi/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/YangyiHuang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/YangyiHuang.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <h2>News</h2>
                <ul>
                  <li><strong>08/2023</strong> We released <a
                      href="https://huangyangyi.github.io/TeCH/">TeCH</a> and <a href="https://tada.is.tue.mpg.de">TADA!</a> on arXiv!
                  <li><strong>07/2023</strong> Our paper <a href="https://huangyangyi.github.io/ELICIT/"><strong>ELICIT</strong></a> is accepted to ICCV 2023!
                  <li><strong>07/2023</strong> Invited to give a talk on <a href="https://ps.is.mpg.de/events/full-body-avatars-from-single-images-and-textual-guidance"><i>
                    Full-body avatars from single images and textual guidance</i></a> at <a href="https://ps.is.tuebingen.mpg.de/">Perceiving Systems, MPI-IS</a> </li>
                  </div>
                </ul>
              </td>
            </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interest lies in computer vision, neural rendering and generative modeling. Currently I mainly focus on creating 3D contents and modelling 3D humans. Representative papers/projects are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    
      <tr onmouseout="tech_stop()" onmouseover="tech_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" style="vertical-align:middle">
            <div class="two" id='tech_video'><video  width=100% height=100% muted autoplay loop>
            <source src="images/tech.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <!-- <img id='tech_image' src="images/tech.png" width=100% style="display: block"> -->
          </div>
          <!-- <script type="text/javascript">
            function tech_start() {
              document.getElementById('tech_video').style.opacity = "1";
              document.getElementById('tech_image').style.opacity = "0";
            }

            function tech_stop() {
              document.getElementById('tech_image').style.opacity = "1";
              document.getElementById('tech_video').style.opacity = "0";
            }
            tech_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://huangyangyi.github.io/TeCH/">
            <span class="papertitle">TeCH: Text-guided Reconstruction of Lifelike Clothed Humans</span>
          </a>
          <br>
          <strong>Yangyi Huang*</strong>,
          <a href="https://xyyhw.top">Hongwei Yi*</a>,
          <a href="https://xiuyuliang.cn">Yuliang Xiu*</a>,
          <a href="https://github.com/TingtingLiao">Tingting Liao</a>,
          <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
          <a href="http://www.cad.zju.edu.cn/home/dengcai/">Deng Cai</a>,
          <a href="https://justusthies.github.io/">Justus Thies</a>
          (* denotes equal contribution)
          <br>
          <em>3DV</em>, 2024
          <br>
          <a href="https://github.com/huangyangyi/TeCH">code</a>
          /
          <a href="https://arxiv.org/abs/2308.08545">arXiv</a>
          /
          <a href="https://huangyangyi.github.io/TeCH/">project</a>
          /
          <a href="https://youtu.be/SjzQ6158Pho">video</a>
          <p></p>
          <p>
          We reconstruct high-resolution textured meshes for clothed humans from single images, with textual guidance from a VQA model and a few-shot finetuned T2I model.
          </p>
        </td>
      </tr>
      
      
      <tr onmouseout="tada_stop()" onmouseover="tada_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" style="vertical-align:middle">
            <div class="two" id='tada_video'><video  width=100% height=100% muted autoplay loop>
            <source src="images/tada.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <!-- <img id='tada_image' src="images/tada.jpg" width=100% style="display: block"> -->
          </div>
          <!-- <script type="text/javascript">
            function tada_start() {
              document.getElementById('tada_video').style.opacity = "1";
              document.getElementById('tada_image').style.opacity = "0";
            }

            function tada_stop() {
              document.getElementById('tada_image').style.opacity = "1";
              document.getElementById('tada_video').style.opacity = "0";
            }
            tada_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://tada.is.tue.mpg.de/">
            <span class="papertitle">TADA! Text to Animatable Digital Avatars</span>
          </a>
          <br>
          <a href="https://github.com/TingtingLiao">Tingting Liao*</a>,
          <a href="https://xyyhw.top">Hongwei Yi*</a>,
          <a href="https://xiuyuliang.cn">Yuliang Xiu</a>,
          <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
          <strong>Yangyi Huang</strong>,
          <a href="https://justusthies.github.io/">Justus Thies</a>,
          <a href="https://ps.is.mpg.de/~black">Michael J. Black</a>
          <br>
          <em>3DV</em>, 2024
          <br>
          <a href="https://github.com/TingtingLiao/TADA">code</a>
          /
          <a href="https://arxiv.org/abs/2308.10899">arXiv</a>
          /
          <a href="https://tada.is.tue.mpg.de/">project</a>
          /
          <a href="https://youtu.be/w5rdcPQWktE">video</a>
          <p></p>
          <p>
            We create expressive 3D avatars from text, based on T2I (I: image; T: text) model.
          </p>
        </td>
      </tr>
      
      <tr onmouseout="elicit_stop()" onmouseover="elicit_start()" bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" style="vertical-align:middle">
            <div class="two" id='elicit_video'><video width=100% height=100% muted autoplay loop>
            <source src="images/elicit.mov" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <!-- <img id='elicit_image' src="images/elicit.png" width=100% style="display: block"> -->
          </div>
          <!-- <script type="text/javascript">
            function elicit_start() {
              document.getElementById('elicit_video').style.opacity = "1";
              document.getElementById('elicit_image').style.opacity = "0";
            }

            function elicit_stop() {
              document.getElementById('elicit_image').style.opacity = "1";
              document.getElementById('elicit_video').style.opacity = "0";
            }
            elicit_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://elicit.is.tue.mpg.de/">
            <span class="papertitle">One-shot Implicit Animatable Avatars with
              Model-based Priors</span>
          </a>
          <br>
          <strong>Yangyi Huang*</strong>,
          <a href="https://xyyhw.top">Hongwei Yi*</a>,
          <a href="https://wyliu.com/">Weiyang Liu</a>,
          <a href="https://haofanwang.github.io/">Haofan Wang</a>,
          <a href="https://scholar.google.com/citations?user=AqDe35sAAAAJ">Boxi Wu</a>,
          <a href="https://www.wenxiaowang.com/">Wenxiao Wang</a>,
          <a href="https://scholar.google.com/citations?user=Zmvq4KYAAAAJ">Binbin Lin</a>,
          <a href="https://www.linkedin.com/in/%E5%BE%B7%E5%85%B5-%E5%BC%A0-a6451aa9?">Debing Zhang</a>,
          <a href="http://www.cad.zju.edu.cn/home/dengcai/">Deng Cai</a>(* denotes equal contribution)
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://github.com/huangyangyi/ELICIT">code</a>
          /
          <a href="https://arxiv.org/abs/2212.02469">arXiv</a>
          /
          <a href="https://huangyangyi.github.io/ELICIT">project</a>
          <p></p>
          <p>
            We create a animatable NeRF-based avatar reconstruction from a single image with model-based prior.
          </p>
        </td>
      </tr>
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/bevformer++.png" width=100% style="display: block">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="BEVFormer++: Improving BEVFormer for 3D Camera-only Object Detection">
            <span class="papertitle">BEVFormer++: Improving BEVFormer for 3D Camera-only Object Detection</span>
          </a>
          <br>
          <a href="https://zhiqi-li.github.io/">Zhiqi Li*</a>, 
          <a href="https://scholar.google.com/citations?hl=en&user=NSeqMYIAAAAJ">Hanming Deng*</a>, 
          <a href="https://github.com/sephyli">Tianyu Li*</a>, 
          <strong>Yangyi Huang*</strong>, 
          <a href="https://github.com/ChonghaoSima">Chonghao Sima</a>*, 
          <a href="https://scholar.google.com/citations?user=GqOe2s0AAAAJ&hl=en">Xiangwei Geng</a>*, 
          <a href="https://scholar.google.com/citations?hl=en&user=s3u33VAAAAAJ">Yulu Gao*</a>, 
          <a href="https://whai362.github.io/">Wenhai Wang*</a>, 
          Yang Li, 
          <a href="https://scholar.google.com/citations?user=zdgKJXIAAAAJ">Lewei Lu</a>
          <br>
          1st place solution for Waymo Open Dataset Challenge 2022
          <br>
          <a href="https://storage.googleapis.com/waymo-uploads/files/research/3DCam/3DCam_BEVFormer.pdf">report</a>
          <p></p>
          <p>
            We enhanced BEVFormer, a DETR-based 3D detection model, with efficient techniques.
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/fuseformer.png" width=100% style="display: block">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="FuseFormer: Fusing Fine-Grained Informationin TransformersforVideoInpainting">
            <span class="papertitle">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting</span>
          </a>
          <br>
          <a href="https://ruiliu-ai.github.io/">Rui Liu*</a>, 
          <a href="https://scholar.google.com/citations?hl=en&user=NSeqMYIAAAAJ">Hanming Deng*</a>, 
          <strong>Yangyi Huang*</strong>, 
          <a href="https://xiaoyushi97.github.io/">Xiaoyu Shi</a>, 
          <a href="https://scholar.google.com/citations?user=zdgKJXIAAAAJ">Lewei Lu</a>
          <a href="http://wenxiusun.com/">Wenxiu Sun</a>, 
          <a href="https://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>, 
          <a href="https://jifengdai.org/">Jifeng Dai</a>, 
          <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
          (* denotes equal contribution)
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://github.com/ruiliu-ai/FuseFormer">code</a>
          /
          <a href="https://arxiv.org/abs/2109.02974">arXiv</a>
          <p></p>
          <p>
            We proposed a Transformer-based model to inpaint videos with sharp and realistic results.
          </p>
        </td>
      </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This site is based on <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>'s template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
